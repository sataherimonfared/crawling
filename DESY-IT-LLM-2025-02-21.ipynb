{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f06297-9915-4acc-b400-f1847cb3881e",
   "metadata": {},
   "source": [
    "### 1- Scrape and Extract Website Data\n",
    "Use BeautifulSoup or Scrapy to scrape text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "390daaa1-3c05-4fd9-8b4d-911403a1685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/taheri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://www.desy.de/index_eng.html - Status Code: 200\n",
      "Extracted 65 meaningful text chunks\n",
      "Fetching https://www.desy.de/news/index_eng.html - Status Code: 200\n",
      "Extracted 4 meaningful text chunks\n",
      "Fetching https://www.desy.de/about_desy/desy/index_eng.html - Status Code: 200\n",
      "Extracted 9 meaningful text chunks\n",
      "Fetching https://www.desy.de/contact/index_eng.html - Status Code: 200\n",
      "Extracted 8 meaningful text chunks\n",
      "Fetching https://www.desy.de/about_desy/directorate/helmut_dosch/index_eng.html - Status Code: 200\n",
      "Extracted 19 meaningful text chunks\n",
      "Fetching https://www.desy.de/research/index_eng.html - Status Code: 200\n",
      "Extracted 13 meaningful text chunks\n",
      "Fetching https://www.desy.de/research/accelerators/index_eng.html - Status Code: 200\n",
      "Extracted 12 meaningful text chunks\n",
      "Fetching https://www.desy.de/research/photon_science/index_eng.html - Status Code: 200\n",
      "Extracted 20 meaningful text chunks\n",
      "Fetching https://www.desy.de/research/particle_physics/index_eng.html - Status Code: 200\n",
      "Extracted 21 meaningful text chunks\n",
      "Fetching https://www.desy.de/research/astroparticle_physics/index_eng.html - Status Code: 200\n",
      "Extracted 50 meaningful text chunks\n",
      "Fetching https://www.desy.de/research/facilities__projects/index_eng.html - Status Code: 200\n",
      "Extracted 63 meaningful text chunks\n",
      "Fetching https://www.desy.de/research/cooperations__institutes/index_eng.html - Status Code: 200\n",
      "Extracted 9 meaningful text chunks\n",
      "Fetching https://www.desy.de/research/facilities__projects/european_xfel/index_eng.html - Status Code: 200\n",
      "Extracted 46 meaningful text chunks\n",
      "Fetching https://www.desy.de/research/facilities__projects/petra_iv/index_eng.html - Status Code: 200\n",
      "Extracted 2 meaningful text chunks\n",
      "Total extracted text chunks: 229\n",
      "Chunk 1: new issue life sciences x-ray source petra iii free-electron laser.\n",
      "\n",
      "Chunk 2: x-ray light source petra iv.\n",
      "\n",
      "Chunk 3: connect.desy.de join the desy alumni network!\n",
      "\n",
      "Chunk 4: url: https://www.desy.de/index_eng.html 123456innovativestrengthlearn more about desy's visionary.\n",
      "\n",
      "Chunk 5: innovative strength learn more about desy's visionary x-ray light source petra iv.\n",
      "\n",
      "Total extracted chunks: 229\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from collections import OrderedDict\n",
    "import difflib\n",
    "\n",
    "\n",
    "def is_similar(a, b, threshold=0.9):\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio() > threshold\n",
    "    \n",
    "# Download NLTK sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "# List of URLs to fetch\n",
    "urls = [\n",
    "    \"https://www.desy.de/index_eng.html\",\n",
    "    \"https://www.desy.de/news/index_eng.html\",\n",
    "    \"https://www.desy.de/about_desy/desy/index_eng.html\",\n",
    "    \"https://www.desy.de/contact/index_eng.html\",\n",
    "    \"https://www.desy.de/about_desy/directorate/helmut_dosch/index_eng.html\",\n",
    "    \"https://www.desy.de/research/index_eng.html\",\n",
    "    \"https://www.desy.de/research/accelerators/index_eng.html\",\n",
    "    \"https://www.desy.de/research/photon_science/index_eng.html\",\n",
    "    \"https://www.desy.de/research/particle_physics/index_eng.html\",\n",
    "    \"https://www.desy.de/research/astroparticle_physics/index_eng.html\",\n",
    "    \"https://www.desy.de/research/facilities__projects/index_eng.html\",\n",
    "    \"https://www.desy.de/research/cooperations__institutes/index_eng.html\",\n",
    "    \"https://www.desy.de/research/facilities__projects/european_xfel/index_eng.html\",\n",
    "    \"https://www.desy.de/research/facilities__projects/petra_iv/index_eng.html\"\n",
    "]\n",
    "\n",
    "all_text_chunks = []\n",
    "# Fetch the webpage\n",
    "#url = \"https://www.desy.de/index_eng.html\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        #Many websites block requests from scripts that don’t look like normal browsers. Try adding a User-Agent to your request:\n",
    "        headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        #response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        continue\n",
    "    print(f\"Fetching {url} - Status Code: {response.status_code}\")\n",
    "    #response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    " \n",
    "    #main_content = soup.find('main')\n",
    "    #main_content = soup.find('div', id='content')  # Example if content is in a div with class=\"content\" \"research-field\"\n",
    "    main_content = soup.find('div', id='content') or soup.find('body')\n",
    "    \n",
    "\n",
    "    #if main_content:\n",
    "    #    print(f\"Extracted main content from {url}\")\n",
    "    #else:\n",
    "    #    print(f\"No <main> tag found in {url}, using full page\")\n",
    "\n",
    "\n",
    "    \n",
    "    if main_content:\n",
    "      #  text = main_content.get_text(strip=True, separator=\" \")  # Extract and clean text\n",
    "        text = main_content.get_text(strip=True, separator=\"\\n\")  # Extract and clean text\n",
    "    else:\n",
    "        text = soup.get_text(strip=True, separator=\" \")  # Fallback to full-page text extraction\n",
    "\n",
    "\n",
    "    if main_content:\n",
    "    # Find all relevant text elements, but only if they are marked as English (lang=\"en\")\n",
    "        tags = main_content.find_all(\n",
    "            ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div', 'li', 'article', 'section', 'td', 'th'],\n",
    "            lang=\"en\")\n",
    "\n",
    "    # If no explicit lang=\"en\" is found, fallback to extracting all tags\n",
    "        if not tags:\n",
    "            tags = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div', 'li', 'article', 'section', 'td', 'th'])\n",
    "\n",
    "    else:\n",
    "        tags = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div', 'li'])\n",
    "\n",
    "\n",
    "    \n",
    "    # if main_content:\n",
    "    #     #tags = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div', 'li'])\n",
    "    #     tags = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div', 'li', 'article', 'section', 'td', 'th'],lang=\"en\")\n",
    "\n",
    "    # else:\n",
    "    #     tags = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div', 'li'])\n",
    "\n",
    "    raw_text = \" \".join(tag.get_text(strip=True) for tag in tags)\n",
    "    #print(f\"Raw extracted text from {url}:\\n{raw_text[:500]}...\\n\")\n",
    "\n",
    "\n",
    "# Extract and clean text\n",
    "# Remove unwanted content (e.g., navigation menu, footer text)\n",
    "#    excluded_keywords = [ \"contact\", \"privacy\", \"terms\", \"login\"] #, \"menu\", \"search\"\n",
    "    excluded_keywords = [\n",
    "    \"contact\", \"privacy\", \"terms\", \"login\", \"menu\", \"search\", \n",
    "    \"subscribe\", \"cookie\", \"policy\", \"newsletter\", \"copyright\", \n",
    "    \"footer\", \"disclaimer\", \"faq\", \"sitemap\"\n",
    "    ]\n",
    "\n",
    "    filtered_tags = [\n",
    "        tag for tag in tags if not any(kw in tag.get_text().lower() for kw in excluded_keywords)\n",
    "    ]\n",
    "    # Clean text    \n",
    "    text = \" \".join(tag.get_text(strip=True) for tag in filtered_tags)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove content inside square brackets\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Remove excess newlines\n",
    "    text = re.sub(r'\\s([?.!,;])', r'\\1', text)  # Fix spacing before punctuation\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1. \\2', text)  # Fix missing spaces between sentences\n",
    "    text = re.sub(r'(?<=[a-zA-Z])\\s*\\n\\s*(?=[a-zA-Z])', '. ', text)  # Convert newlines to periods if missing punctuation\n",
    "\n",
    "    # Split text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "\n",
    "    #print(\"===============\")\n",
    "    #print(\"Sample tokenized sentences:\", sentences[:5])\n",
    "    #print(\"===============\")\n",
    "    # Filter out very short sentences\n",
    "    chunks = [sentence.strip() for sentence in sentences if len(sentence.split()) > 3]  # Adjust the length as needed\n",
    "\n",
    "\n",
    "\n",
    "    # Normalize sentences (lowercase, strip extra spaces)\n",
    "    normalized_chunks = list(set(sentence.lower().strip() for sentence in chunks))\n",
    "\n",
    "\n",
    "    # Remove near-duplicates (keep only one version)\n",
    "    filtered_chunks = []\n",
    "    for sentence in normalized_chunks:\n",
    "        if not any(is_similar(sentence, existing) for existing in filtered_chunks):\n",
    "            filtered_chunks.append(sentence)\n",
    "\n",
    "    # # Convert to a set to remove duplicates\n",
    "    # unique_chunks = list(set(chunks))\n",
    "    # # Sort by original order (optional, to maintain readability)\n",
    "    # unique_chunks.sort(key=lambda x: chunks.index(x))\n",
    "    # # Append extracted unique chunks\n",
    "    # all_text_chunks.extend(unique_chunks)\n",
    "    # print(f\"Extracted {len(unique_chunks)} unique text chunks\")\n",
    "\n",
    "    # Preserve original order while ensuring uniqueness\n",
    "    unique_chunks = list(OrderedDict.fromkeys(filtered_chunks))  # Removes duplicates while keeping order\n",
    "\n",
    "    #Append extracted unique chunks\n",
    "    all_text_chunks.extend(unique_chunks)\n",
    "    \n",
    "    # Append extracted chunks to the main list\n",
    "    #all_text_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"Extracted {len(chunks)} meaningful text chunks\")\n",
    "\n",
    "\n",
    "print(f\"Total extracted text chunks: {len(all_text_chunks)}\")\n",
    "for i, chunk in enumerate(all_text_chunks[:5]):  # Print first 5 chunks\n",
    "    print(f\"Chunk {i+1}: {chunk}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Save chunks to a text file\n",
    "with open(\"desy_content.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in all_text_chunks:\n",
    "        f.write(chunk + \"\\n\")\n",
    "\n",
    "if not all_text_chunks:\n",
    "    raise ValueError(\"No text chunks were extracted from the URLs.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total extracted chunks: {len(all_text_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0043b13-6be7-4800-bd1b-eeed2f32fa4f",
   "metadata": {},
   "source": [
    "### 2-Convert Data into Embeddings (Vector Database)\n",
    "Once we collect DESY’s text, we store it in a vector database for fast retrieval.\n",
    "- Use OpenAI’s text-embedding-ada-002 or Hugging Face models (all-MiniLM-L6-v2).\n",
    "- Store the vectors in FAISS, Pinecone, or ChromaDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea368ea9-9a61-4e9d-af87-cc433038c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Set environment variable to reduce memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Step 1: Load the FAISS index (if it exists)\n",
    "# FAISS Index Directory\n",
    "faiss_index_path = \"desy_faiss_index\"\n",
    "os.makedirs(faiss_index_path, exist_ok=True)\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vectorstore = FAISS.from_texts(all_text_chunks, embeddings)\n",
    "vectorstore.save_local(\"desy_faiss_index\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3bf96-3cb6-4e7b-af06-5556934d5700",
   "metadata": {},
   "source": [
    "### 3-Build the RAG Pipeline (LLM + Retrieval)\n",
    "When a user asks a question, we:\n",
    "- **Retrieve relevant DESY documents** from the vector database.\n",
    "- **Feed them into an LLM** (LLaMA 2, GPT, or Mistral) to generate responses.\n",
    "\n",
    "\n",
    "Example: Retrieval + LLM Response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ce9642-4ccc-4640-9bc7-ed6265e1c3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_111870/1096887078.py:37: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=llama_model)\n",
      "/tmp/ipykernel_111870/1096887078.py:87: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_with_data({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many employees does DESY have?\n",
      "===================================\n",
      "Answer with RAG: DESY has approximately 3000 employees.\n",
      "\n",
      "Question: What is DESY\n",
      "===================================\n",
      "Answer without RAG: 1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Step 2: Load the Llama Model\n",
    "model_name = \"openlm-research/open_llama_3b\"\n",
    "#model_name = \"openlm-research/open_llama_3b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  # Use fast tokenizer if available\n",
    "device = \"cpu\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)  # Use FP16\n",
    "\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "\n",
    "llama_model = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    #max_length=512,  # 128,256,512 for faster/slower inference & to control cost\n",
    "    #max_new_tokens=500,  # Ensures short answers\n",
    "    temperature=1.0,  # [0-2] set randomness- increase for more creative answers like image/poem generator-decrease for more precise answer for fact based models\n",
    "    top_p=1.0,  # [0-1] Ensures only high-probability tokens are used- It controls how deterministic the model is. \n",
    "    #top_k=10,\n",
    "    #do_sample=False,\n",
    "    #num_return_sequences=1 #,\n",
    "    #stop_sequence=\"\\n\"  # Stop generating after a newline\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llama_model)\n",
    "\n",
    "prompt_template = \"\"\"Answer the question based on the context provided below. If the context does not contain the answer, say \"I don't know.\" \n",
    "- Make sure to check **all parts of the context** carefully before answering, even if the answer is spread across multiple sections.\n",
    "- Do **not** repeat words or phrases.\n",
    "- Provide a **complete and well-structured sentence** as your answer.\n",
    "- If the question asks for multiple points, provide a **list** or **detailed explanation**.\n",
    "- If the answer requires interpretation or synthesis of multiple pieces of information, ensure that the answer reflects the entire context accurately.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "#print(\"Final Context to LLM:\")\n",
    "#for doc in retrieved_docs:\n",
    "#    print(doc.page_content)\n",
    "\n",
    "\n",
    "# Step 3: Build the RAG pipeline\n",
    "qa_with_data = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Use \"map_reduce\" or \"refine\" for more complex tasks\n",
    "    #retriever=vectorstore.as_retriever(search_kwargs={\"k\": 10, \"score_threshold\": 0.25, \"search_type\": \"similarity\"}),  # Adjust k based on document length #score_threshold to filter out low-confidence retrievals\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 30, \"search_type\": \"mmr\"}), #, \"score_threshold\": 0.3   \n",
    "    return_source_documents=True , # Optionally return source documents for debugging\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "\n",
    "# Query the pipeline\n",
    "query = \"How many employees does DESY have?\"\n",
    "#query = \"What are the main research fields at DESY?\"\n",
    "#query = \"Who is the Chairman of the DESY Board of Directors?\"\n",
    "#query = \"Who funds DESY, and how is it managed?\"\n",
    "#quary = \"What is European XFEL, and how is DESY involved?\"\n",
    "#query=\"How does DESY contribute to particle physics?\"\n",
    "\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"===================================\")\n",
    "\n",
    "result = qa_with_data({\"query\": query})\n",
    "# Post-process the output to extract only the answer\n",
    "answer = result['result'].split(\"Answer:\")[-1].strip()\n",
    "print(f\"Answer with RAG: {answer}\")\n",
    "\n",
    "print(\"===================================\")\n",
    "\n",
    "formatted_prompt = PROMPT.format(context=\"No context available.\", question=query)\n",
    "response_without_data = llama_model(formatted_prompt, num_return_sequences=1, do_sample=False)\n",
    "\n",
    "\n",
    "#response_without_data = llama_model(query, num_return_sequences=1, do_sample=False) #max_length=256, truncation=True\n",
    "print(\"Answer without RAG:\", response_without_data[0][\"generated_text\"].split(\"Answer:\")[-1].strip())\n",
    "\n",
    "#print(\"===================================\")\n",
    "#print(f\"Source Documents: {result['source_documents']}\")\n",
    "\n",
    "#scoring function? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc0c85e4-8a78-415c-a60c-989b6fd2925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git version 2.43.5\n"
     ]
    }
   ],
   "source": [
    "!git --version\n",
    "!git config --global user.email \"taheri@mail.desy.de\"\n",
    "!git config --global user.name \"taheri\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb8561-2def-4c9d-b47f-c3594f536656",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh-keygen -t rsa -b 4096 -C \"taheri@mail.desy.de\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f7330c-748f-446c-b5ed-714d6b376fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/.ssh/id_rsa.pub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebc135c3-223a-430d-b6fd-ab5e58f4c57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in /home/taheri/.git/\n",
      "[master (root-commit) 10bf5a6] Initial commit\n",
      " 1 file changed, 582 insertions(+)\n",
      " create mode 100644 DESY-IT-LLM-2025-02-21.ipynb\n",
      "error: remote origin already exists.\n",
      "error: src refspec main does not match any\n",
      "\u001b[31merror: failed to push some refs to 'https://github.com/username/repo.git'\n",
      "\u001b[m"
     ]
    }
   ],
   "source": [
    "\n",
    "!git init\n",
    "!git add DESY-IT-LLM-2025-02-21.ipynb\n",
    "!git commit -m \"Initial commit\"\n",
    "!git remote add origin https://gitlab.desy.de/taheri/repo.git\n",
    "!git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9db14-7a4a-4e49-833a-a03a165d07c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma-LLMDESY",
   "language": "python",
   "name": "chroma-llmdesy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
