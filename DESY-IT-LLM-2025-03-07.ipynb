{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f06297-9915-4acc-b400-f1847cb3881e",
   "metadata": {},
   "source": [
    "### 1- Scrape and Extract Website Data\n",
    "Use BeautifulSoup or Scrapy to scrape text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784d8b6-abf4-41d0-af10-6d5795e917f5",
   "metadata": {},
   "source": [
    "## BeautifulSoup Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4cdb32-3adf-48e4-a381-ea0853e50323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/taheri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://www.desy.de/news/index_eng.html - Status Code: 200\n",
      "Extracted 3 meaningful text chunks from https://www.desy.de/news/index_eng.html\n",
      "Fetching https://www.desy.de/about_desy/desy/index_eng.html - Status Code: 200\n",
      "Fetching https://www.desy.de/contact/index_eng.html - Status Code: 200\n",
      "Fetching https://www.desy.de/index_eng.html - Status Code: 200\n",
      "Extracted 8 meaningful text chunks from https://www.desy.de/about_desy/desy/index_eng.html\n",
      "Extracted 7 meaningful text chunks from https://www.desy.de/contact/index_eng.html\n",
      "Fetching https://www.desy.de/research/index_eng.html - Status Code: 200\n",
      "Fetching https://www.desy.de/about_desy/directorate/helmut_dosch/index_eng.html - Status Code: 200\n",
      "Extracted 8 meaningful text chunks from https://www.desy.de/research/index_eng.html\n",
      "Extracted 8 meaningful text chunks from https://www.desy.de/about_desy/directorate/helmut_dosch/index_eng.html\n",
      "Fetching https://www.desy.de/research/accelerators/index_eng.html - Status Code: 200\n",
      "Fetching https://www.desy.de/research/photon_science/index_eng.html - Status Code: 200\n",
      "Extracted 33 meaningful text chunks from https://www.desy.de/index_eng.html\n",
      "Extracted 11 meaningful text chunks from https://www.desy.de/research/accelerators/index_eng.html\n",
      "Fetching https://www.desy.de/research/particle_physics/index_eng.html - Status Code: 200\n",
      "Fetching https://www.desy.de/research/astroparticle_physics/index_eng.html - Status Code: 200\n",
      "Extracted 17 meaningful text chunks from https://www.desy.de/research/particle_physics/index_eng.html\n",
      "Fetching https://www.desy.de/research/facilities__projects/index_eng.html - Status Code: 200\n",
      "Extracted 16 meaningful text chunks from https://www.desy.de/research/photon_science/index_eng.html\n",
      "Fetching https://www.desy.de/research/cooperations__institutes/index_eng.html - Status Code: 200\n",
      "Extracted 5 meaningful text chunks from https://www.desy.de/research/cooperations__institutes/index_eng.html\n",
      "Fetching https://www.desy.de/research/facilities__projects/european_xfel/index_eng.html - Status Code: 200\n",
      "Extracted 27 meaningful text chunks from https://www.desy.de/research/facilities__projects/index_eng.html\n",
      "Extracted 34 meaningful text chunks from https://www.desy.de/research/astroparticle_physics/index_eng.html\n",
      "Extracted 31 meaningful text chunks from https://www.desy.de/research/facilities__projects/european_xfel/index_eng.html\n",
      "Fetching https://www.desy.de/research/facilities__projects/petra_iv/index_eng.html - Status Code: 200\n",
      "Extracted 2 meaningful text chunks from https://www.desy.de/research/facilities__projects/petra_iv/index_eng.html\n",
      "Chunk 1: URL:... Breadcrumb Navigation Home / News News News and events News and events aktuelles news 1 1 10 Narrow 0,1|4,1|5, 0 0 1 news_suche eng 1 1 8 both 0 1 %Y/%m/%d Press-Release more news more news more news Events - 15:00The 626th XFEL PHOTON BEAM SYSTEMS meeting - 10:30The recent detection of an UHE neutrino by KM3Ne.\n",
      "\n",
      "Chunk 2: T - 15:00The 627th XFEL PHOTON BEAM SYSTEMS meetingmore events Events - 15:00The 626th XFEL PHOTON BEAM SYSTEMS meeting - 10:30The recent detection of an UHE neutrino by KM3Ne.\n",
      "\n",
      "Chunk 3: T - 10:30 - 15:00The 627th XFEL PHOTON BEAM SYSTEMS meeting - 15:00 more events more events more events\n",
      "\n",
      "Chunk 4: URL:... Breadcrumb Navigation Home / About DESY / DESY DESY The decoding of matter The decoding of matter Large-scale facilities Innovation.\n",
      "\n",
      "Chunk 5: Without inventions and catchy ideas, new and commercially successful...\n",
      "\n",
      "Total extracted chunks: 210\n"
     ]
    }
   ],
   "source": [
    "import requests #Used to send HTTP requests to fetch web pages\n",
    "from bs4 import BeautifulSoup #Parses HTML content to extract useful information.\n",
    "import re #Handles regular expressions for text cleaning.\n",
    "import nltk #Natural Language Toolkit for text processing (e.g., sentence tokenization)\n",
    "from collections import OrderedDict \n",
    "import difflib #Compares text similarity.\n",
    "from cleantext import clean\n",
    "import ftfy #Fixes Unicode issues in text.\n",
    "from unidecode import unidecode #Converts Unicode text to ASCII.\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed #Enables parallel processing \n",
    "\n",
    "# Function to check similarity\n",
    "def is_similar(a, b, threshold=0.9): #still needs to be tuned\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio() > threshold\n",
    "\n",
    "# Download NLTK sentence tokenizer which is used to split text into sentences.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# List of URLs to fetch\n",
    "urls = [\n",
    "    \"https://www.desy.de/index_eng.html\",\n",
    "    \"https://www.desy.de/news/index_eng.html\",\n",
    "    \"https://www.desy.de/about_desy/desy/index_eng.html\",\n",
    "    \"https://www.desy.de/contact/index_eng.html\",\n",
    "    \"https://www.desy.de/about_desy/directorate/helmut_dosch/index_eng.html\",\n",
    "    \"https://www.desy.de/research/index_eng.html\",\n",
    "    \"https://www.desy.de/research/accelerators/index_eng.html\",\n",
    "    \"https://www.desy.de/research/photon_science/index_eng.html\",\n",
    "    \"https://www.desy.de/research/particle_physics/index_eng.html\",\n",
    "    \"https://www.desy.de/research/astroparticle_physics/index_eng.html\",\n",
    "    \"https://www.desy.de/research/facilities__projects/index_eng.html\",\n",
    "    \"https://www.desy.de/research/cooperations__institutes/index_eng.html\",\n",
    "    \"https://www.desy.de/research/facilities__projects/european_xfel/index_eng.html\",\n",
    "    \"https://www.desy.de/research/facilities__projects/petra_iv/index_eng.html\"\n",
    "    # \"https://it.desy.de/index_eng.html\",\n",
    "    # \"https://it.desy.de/availability/index_eng.html\",\n",
    "    # \"https://it.desy.de/help_uco/index_eng.html\",\n",
    "    # \"https://it.desy.de/e16/e2036/e55767/index_eng.html?preview=preview\"\n",
    "]\n",
    "\n",
    "# Function to fetch and clean webpage content\n",
    "def fetch_and_clean_url(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Fetching {url} - Status Code: {response.status_code}\")\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract main content- fallback mechanism!\n",
    "# def extract_main_content(soup):\n",
    "#     for tag in soup(['nav', 'footer', 'script', 'style', 'aside', 'header', 'form', 'iframe', 'img']):# Remove unwanted tags (e.g., nav, footer, script, style)\n",
    "#         tag.decompose()\n",
    "#     return soup.find('div', id='content') or soup.find('body') #body tag contains all the visible content of the webpage\n",
    "\n",
    "def extract_main_content(soup):\n",
    "    for tag in soup(['nav', 'footer', 'script', 'style', 'aside', 'header', 'form', 'iframe', 'img']):\n",
    "        tag.decompose()\n",
    "    \n",
    "    # Prioritize content divs\n",
    "    main_content = soup.find('div', id='content') or soup.find('main') or soup.find('body')\n",
    "    \n",
    "    if main_content:\n",
    "        return main_content\n",
    "    else:\n",
    "        return soup  # Fallback to entire page if no main content is found\n",
    "\n",
    "# Function to filter tags based on exclusion keywords\n",
    "def filter_tags(tags, excluded_keywords):\n",
    "    return [tag for tag in tags if not any(kw in tag.get_text().lower() for kw in excluded_keywords)]\n",
    "\n",
    "# Function to clean the extracted text\n",
    "def clean_text(raw_text):\n",
    "    raw_text = ftfy.fix_text(raw_text)  # Fix Unicode issues\n",
    "    raw_text = unidecode(raw_text)  # Convert to ASCII\n",
    "    raw_text = re.sub(r'http\\S+', '', raw_text)  # Remove URLs\n",
    "    raw_text = re.sub(r'\\d{2}\\.\\d{2}\\.\\d{4}', '', raw_text)  # Remove dates\n",
    "  # raw_text = re.sub(r'\\b\\w{1,2}\\b', '', raw_text)  # Remove single or double-letter words\n",
    "\n",
    "    raw_text = re.sub(r'\\s+', ' ', raw_text)  # Normalize spaces\n",
    "    raw_text = re.sub(r'\\[.*?\\]', '', raw_text)  # Remove content inside square brackets\n",
    "    raw_text = re.sub(r'\\n+', '\\n', raw_text)  # Remove excess newlines\n",
    "    raw_text = re.sub(r'\\s([?.!,;])', r'\\1', raw_text)  # Fix spacing before punctuation\n",
    "    \n",
    "    raw_text = re.sub(r'([a-z])([A-Z])', r'\\1. \\2', raw_text)  # Fix missing spaces between sentences\n",
    "    raw_text = re.sub(r'(?<=[a-zA-Z])\\s*\\n\\s*(?=[a-zA-Z])', '. ', raw_text)  # Convert newlines to periods if missing punctuation\n",
    "    return raw_text.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_chunks(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = [sentence.strip() for sentence in sentences if len(sentence.split()) > 5]  # Keep only meaningful sentences\n",
    "    \n",
    "    unique_chunks = OrderedDict()  # Maintain order while removing duplicates\n",
    "    for sentence in chunks:\n",
    "        normalized_sentence = sentence.lower().strip()\n",
    "        if not any(is_similar(normalized_sentence, existing) for existing in unique_chunks):\n",
    "            unique_chunks[normalized_sentence] = sentence  # Store original sentence\n",
    "    \n",
    "    return list(unique_chunks.values())  # Return unique, meaningful chunks\n",
    "\n",
    "#===============================================================================\n",
    "# Function to handle the entire extraction process for a URL\n",
    "def extract_text_from_url(url):\n",
    "    content = fetch_and_clean_url(url)\n",
    "    if content:\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        main_content = extract_main_content(soup)\n",
    "        if main_content:\n",
    "            tags = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div', 'li', 'article', 'section', 'td', 'th'], lang=\"en\")\n",
    "            if not tags:\n",
    "                tags = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div', 'li', 'article', 'section', 'td', 'th'])\n",
    "            raw_text = \" \".join(tag.get_text(strip=True) for tag in filter_tags(tags, excluded_keywords))\n",
    "            cleaned_text = clean_text(raw_text)\n",
    "            return process_chunks(cleaned_text)\n",
    "    return []\n",
    "\n",
    "# Initialize a list to store all the extracted chunks\n",
    "all_text_chunks = []\n",
    "excluded_keywords = [\n",
    "    \"contact\", \"privacy\", \"terms\", \"login\", \"menu\", \"search\", \n",
    "    \"subscribe\", \"cookie\", \"policy\", \"newsletter\", \"copyright\", \n",
    "    \"footer\", \"disclaimer\", \"faq\", \"sitemap\"\n",
    "]\n",
    "\n",
    "# Function to process URLs in parallel\n",
    "def extract_text_from_urls(urls):\n",
    "    all_text_chunks = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:  # Adjust max_workers based on your system\n",
    "        # Submit tasks to the executor\n",
    "        future_to_url = {executor.submit(extract_text_from_url, url): url for url in urls}\n",
    "        \n",
    "        # Process completed tasks\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                unique_chunks = future.result()\n",
    "                all_text_chunks.extend(unique_chunks)\n",
    "                print(f\"Extracted {len(unique_chunks)} meaningful text chunks from {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {url}: {e}\")\n",
    "    return all_text_chunks\n",
    "\n",
    "# Use parallel processing to extract text from all URLs\n",
    "all_text_chunks = extract_text_from_urls(urls)\n",
    "\n",
    "# Final output\n",
    "\n",
    "for i, chunk in enumerate(all_text_chunks[:5]):  # Print first 5 chunks\n",
    "    print(f\"Chunk {i+1}: {chunk}\\n\")\n",
    "\n",
    "# Save chunks to a text file\n",
    "with open(\"desy_content.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in all_text_chunks:\n",
    "        f.write(chunk + \"\\n\")\n",
    "\n",
    "if not all_text_chunks:\n",
    "    raise ValueError(\"No text chunks were extracted from the URLs.\")\n",
    "\n",
    "print(f\"Total extracted chunks: {len(all_text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5333c02-5650-4eec-9848-be608bb5ca99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0043b13-6be7-4800-bd1b-eeed2f32fa4f",
   "metadata": {},
   "source": [
    "### 2-Convert Data into Embeddings (Vector Database)\n",
    "Once we collect DESY’s text, we store it in a vector database for fast retrieval.\n",
    "- Use OpenAI’s text-embedding-ada-002 or Hugging Face models (all-MiniLM-L6-v2).\n",
    "- Store the vectors in FAISS, Pinecone, KDB, or ChromaDB.\n",
    "\n",
    "| **Tool**       | **Performance** | **Scalability** | **Ease of Use** | **Cost** | **Support for Distance Metrics** | **Cloud Integration** |\n",
    "|----------------|-----------------|-----------------|-----------------|----------|----------------------------------|-----------------------|\n",
    "| **FAISS**      | Very Fast (especially for large datasets) | High (needs setup for very large data) | Medium (requires setup, more control) | Free (open-source) | Cosine, Euclidean, Inner product, etc. | Works with local or cloud setups |\n",
    "| **Pinecone**   | Fast (optimized for similarity search) | Very High (highly scalable) | Very Easy (cloud-based, API-driven) | Pay-as-you-go (expensive for large datasets) | Cosine, Euclidean, Dot product, etc. | Cloud-based (fully managed service) |\n",
    "| **KDB**        | Fast (best for time-series data, but can be used for vector search) | Very High (designed for large datasets) | Medium (requires more setup for vector storage) | Expensive (enterprise-focused) | Custom metrics (can handle various types) | Cloud-based (supports large-scale data) |\n",
    "| **ChromaDB**   | Fast (optimized for embeddings, especially with vector search) | Medium (good scalability but less than Pinecone) | Easy (cloud and local integration) | Free with limited usage, Paid for large scale | Cosine, Euclidean, Inner product | Both cloud and local setups |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f31ff-acc1-4465-8c8d-7561f78e41bc",
   "metadata": {},
   "source": [
    "\n",
    "**GOAL :** evaluates the performance of multiple embedding models (both Hugging Face and OpenAI models) \n",
    "Cosine similarity method: It measures how close the embeddings of the query and documents are in the vector space. \n",
    "\n",
    "**What I learned:** \n",
    "- This doesn't always correlate with the quality of the retrieved documents for a specific task.\n",
    "- Some models are better suited for specific tasks. For example, all-mpnet-base-v2 is a general-purpose model, while multi-qa-mpnet-base-dot-v1 is optimized for question-answering tasks.\n",
    "\n",
    "**How to improve:**\n",
    "- Instead of relying solely on cosine similarity, evaluate the models based on task-specific metrics (e.g., Precision@K, Recall@K, Mean Reciprocal Rank (MRR), NDCG, or F1 score for document retrieval)\n",
    "- Embedding models fine tuning on our specific dataset to improve their performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d621e1-6bbc-4002-a2b1-7eaa41b4a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: text-davinci-003\n",
      "Model: text-davinci-003 -- Query: how many employees does desy have?\n",
      "-- Normalized Average Top-5 Document Similarity Score: 0.56\n",
      "\n",
      "===============================\n",
      "Processing model: text-embedding-ada-002\n",
      "Model: text-embedding-ada-002 -- Query: how many employees does desy have?\n",
      "-- Normalized Average Top-5 Document Similarity Score: 0.56\n",
      "\n",
      "===============================\n",
      "Processing model: multi-qa-mpnet-base-dot-v1\n",
      "Model: multi-qa-mpnet-base-dot-v1 -- Query: how many employees does desy have?\n",
      "-- Normalized Average Top-5 Document Similarity Score: 0.61\n",
      "\n",
      "===============================\n",
      "Processing model: all-MiniLM-L6-v2\n",
      "Model: all-MiniLM-L6-v2 -- Query: how many employees does desy have?\n",
      "-- Normalized Average Top-5 Document Similarity Score: 0.52\n",
      "\n",
      "===============================\n",
      "Processing model: msmarco-distilbert-base-v4\n",
      "Model: msmarco-distilbert-base-v4 -- Query: how many employees does desy have?\n",
      "-- Normalized Average Top-5 Document Similarity Score: 0.43\n",
      "\n",
      "===============================\n",
      "Processing model: all-mpnet-base-v2\n",
      "Model: all-mpnet-base-v2 -- Query: how many employees does desy have?\n",
      "-- Normalized Average Top-5 Document Similarity Score: 0.47\n",
      "\n",
      "===============================\n",
      "Processing model: paraphrase-MiniLM-L6-v2\n",
      "Model: paraphrase-MiniLM-L6-v2 -- Query: how many employees does desy have?\n",
      "-- Normalized Average Top-5 Document Similarity Score: 0.39\n",
      "\n",
      "===============================\n",
      "Processing model: distiluse-base-multilingual-cased-v1\n",
      "Model: distiluse-base-multilingual-cased-v1 -- Query: how many employees does desy have?\n",
      "-- Normalized Average Top-5 Document Similarity Score: 0.2\n",
      "\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "# import torch\n",
    "import random\n",
    "# import os\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from urllib.request import urlretrieve\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.helmholtz-blablador.fz-juelich.de/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"glpat-QJocA6joz1gm7XD8rrRo\"\n",
    "\n",
    "\n",
    "# Free up GPU memory if using CUDA\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Define a list of queries\n",
    "queries = [\n",
    "    \"How many employees does DESY have?\"\n",
    "   # \"What are the main research fields at DESY?\",\n",
    "   # \"Who is the Chairman of the DESY Board of Directors?\",\n",
    "   # \"Who is head of IT department?\",\n",
    "   # \"What is European XFEL, and how is DESY involved?\",\n",
    "   # \"How does DESY contribute to particle physics?\"\n",
    "]\n",
    "\n",
    "# Function to preprocess text for consistency -> ensures fair comparisons by removing inconsistencies like capitalization and extra whitespace.\n",
    "def preprocess_text(text):\n",
    "    return text.strip().lower()  # Lowercasing and trimming spaces\n",
    "\n",
    "# Apply preprocessing to each query in the list\n",
    "queries = [preprocess_text(query) for query in queries]\n",
    "\n",
    "\n",
    "\n",
    "# Define a test query\n",
    "#query = \"What are the main research fields at DESY?\"\n",
    "\n",
    "\n",
    "# Apply preprocessing to the query\n",
    "#query = preprocess_text(query)\n",
    "\n",
    "# Use a subset of all_text_chunks to reduce memory usage (e.g., 100 random chunks)\n",
    "subset_text_chunks = random.sample(all_text_chunks, 100)\n",
    "subset_text_chunks = [preprocess_text(text) for text in subset_text_chunks]\n",
    "\n",
    "# List of Hugging Face and OpenAI models to evaluate\n",
    "\n",
    "models = [\n",
    "    \"text-davinci-003\", # OpenAI\n",
    "    \"text-embedding-ada-002\", # OpenAI \n",
    "    \"multi-qa-mpnet-base-dot-v1\", #very sensitive to normalization & For multilingual support\n",
    "    \"all-MiniLM-L6-v2\", #nor\n",
    "    \"msmarco-distilbert-base-v4\", #nor\n",
    "    \"all-mpnet-base-v2\",\n",
    "    \"paraphrase-MiniLM-L6-v2\",\n",
    "    \"distiluse-base-multilingual-cased-v1\" #nor\n",
    "    #\"GritLM-7B\",  #\"alias-embeddings\",\n",
    "    #\"gpt-3.5-turbo\" # (from OpenAI-bot not designed for embedding-gets the same score as text-embedding-ada-002)\n",
    "]\n",
    "\n",
    "# Iterate over each model\n",
    "for model_name in models:\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "\n",
    "    # Create an embedding instance for the current model\n",
    "    if model_name in [\"text-davinci-003\", \"text-embedding-ada-002\"]:\n",
    "        embeddings = OpenAIEmbeddings(model=model_name)\n",
    "    else:\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "\n",
    "\n",
    "        # Iterate over each query\n",
    "    for query in queries:\n",
    "        #print(f\"Evaluating query: {query}\")\n",
    "\n",
    "        # Compute the normalized query embedding\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "        query_embedding = normalize([query_embedding])[0]\n",
    "\n",
    "        # Build a FAISS index from the text chunks\n",
    "        vectorstore = FAISS.from_texts(subset_text_chunks, embeddings)\n",
    "\n",
    "        # Perform a similarity search for the query (retrieve the top 5 documents)\n",
    "        docs = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "        # Compute the average cosine similarity for the top 5 retrieved documents\n",
    "        similarity_scores = []\n",
    "        for doc in docs:\n",
    "            doc_content = preprocess_text(doc.page_content)\n",
    "            \n",
    "            # Get the non-normalized embedding for the document\n",
    "            doc_embedding = embeddings.embed_query(doc_content)\n",
    "\n",
    "            # Normalize the document embedding\n",
    "            doc_embedding_normalized = normalize([doc_embedding])[0]\n",
    "\n",
    "            # Compute the cosine similarity between the query and the document\n",
    "            similarity = cosine_similarity([query_embedding], [doc_embedding_normalized])[0][0]\n",
    "            similarity_scores.append(similarity)\n",
    "\n",
    "        # Calculate the average similarity score for the query\n",
    "        avg_similarity_score = sum(similarity_scores) / len(similarity_scores)\n",
    "\n",
    "        \n",
    "        print(f\"Model: {model_name} -- Query: {query}\\n-- Normalized Average Top-5 Document Similarity Score: {round(avg_similarity_score, 2)}\\n\")\n",
    "    \n",
    "        # Clean up FAISS index to free memory\n",
    "        del vectorstore\n",
    "        torch.cuda.empty_cache()  # Clear GPU memory if applicable\n",
    "\n",
    "    print(\"===============================\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8115d2c-dd0d-4748-8f71-34aa3de58dc7",
   "metadata": {},
   "source": [
    "## Creating and storing vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "173ecb19-04a4-47dc-92b9-dd38e9ce7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "#from urllib.request import urlretrieve\n",
    "#from langchain_openai import ChatOpenAI\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# You need to set them as environment variables, because the OpenAI API client uses them\n",
    "# multiple times\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.helmholtz-blablador.fz-juelich.de/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"glpat-QJocA6joz1gm7XD8rrRo\"\n",
    "\n",
    "\n",
    "# Set environment variable to reduce memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Step 1: Load the FAISS index (if it exists)\n",
    "# FAISS Index Directory\n",
    "faiss_index_path = \"desy_faiss_index\"\n",
    "os.makedirs(faiss_index_path, exist_ok=True)\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "##embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "#embeddings = OpenAIEmbeddings(model=\"text-davinci-003\") \n",
    "#embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\") \n",
    "\n",
    "# Create and save the vector store\n",
    "vectorstore = FAISS.from_texts(all_text_chunks, embeddings)\n",
    "vectorstore.save_local(faiss_index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3bf96-3cb6-4e7b-af06-5556934d5700",
   "metadata": {},
   "source": [
    "### 3-Build the RAG Pipeline (LLM + Retrieval)\n",
    "When a user asks a question, we:\n",
    "- **Retrieve relevant DESY documents** from the vector database.\n",
    "- **Feed them into an LLM** (LLaMA 2, GPT, or Mistral) to generate responses.\n",
    "\n",
    "\n",
    "Example: Retrieval + LLM Response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78d4a3-2a32-46d9-b031-75905902fca9",
   "metadata": {},
   "source": [
    "| **Scenario**               | **Use \"stuff\"** | **Use \"refine\"** | **Use \"map_reduce\"** |\n",
    "|----------------------------|---------------|---------------|----------------|\n",
    "| Short documents            | ✅ | ❌ | ❌ |\n",
    "| Long documents             | ❌ (token limit issue) | ✅ | ✅ |\n",
    "| Needs detailed reasoning   | ❌ | ✅ (step-by-step) | ❌ |\n",
    "| Needs summarization        | ❌ | ❌ | ✅ |\n",
    "| Avoiding bias from a single doc | ❌ | ❌ | ✅ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f26239-2730-4fa2-b676-e067fecc60ad",
   "metadata": {},
   "source": [
    "## chain_type= \"stuff\"\n",
    "how the retrieved documents are combined and processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3adeece9-2e3a-4d53-84cb-c9738403ba4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openlm-research/open_llama_3b\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question: How many employees does DESY have?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Answer with RAG: DESY has approximately 3000 employees.\n",
      "===================================\n",
      "Answer without RAG: 1000\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question: What are the main research fields at DESY?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Answer with RAG: The main research fields at DESY are particle physics, photon science, neutrino physics, and astroparticle physics.\n",
      "===================================\n",
      "Answer without RAG: - Particle physics\n",
      "- Astroparticle physics\n",
      "- Nuclear physics\n",
      "- Astrophysics\n",
      "- Computational physics\n",
      "- Medical physics\n",
      "- Industrial physics\n",
      "-\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question: Who is the Chairman of the DESY Board of Directors?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Answer with RAG: The Chairman of the DESY Board of Directors is Prof. Dr. Dr. h.c. Helmut Dosch.\n",
      "===================================\n",
      "Answer without RAG: The DESY Board of Directors is responsible for the overall strategic direction of DESY.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question: Who funds DESY, and how is it managed?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Answer with RAG: DESY is a member of the Helmholtz Association, which is a federation of 19 German research centers. The Helmholtz Association is funded primarily by the federal government of Germany. DESY is managed by a Board of Directors, which is responsible for setting the organization's overall direction, implementing strategies, and ensuring efficient operations. The Chairman of the Board of Directors, Prof. Dr. Dr. h.c. Helmut Dosch, plays a key role in the management of DESY.\n",
      "===================================\n",
      "Answer without RAG: The DESY laser is a powerful laser that is used to study the properties of matter.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question: What is European XFEL, and how is DESY involved?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Answer with RAG: European XFEL is a X-ray Free-Electron Laser facility, which is Europe's big X-ray laser. DESY is the main shareholder and operates the accelerator of the facility, with the involvement of eleven other countries in the project.\n",
      "===================================\n",
      "Answer without RAG: DESY is a research institute in Germany that is involved in the project.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question: How does DESY contribute to particle physics?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Answer with RAG: DESY contributes to particle physics by being involved in the Belle II experiment at the Super, developing, operating and utilizing state-of-the-art accelerator facilities, designing and construction of extremely sensitive pixel vertex detector, exploring various options for developing theories, playing a leading role in the Radio Neutrino Observatory in Greenland (RNO-G) and the Ice. Cube collaboration, participating in the ULTRASAT satellite mission, and driving the design, development and upgradeas well as the data analysis of Ice. Cube-Gen2, the neutrino observatory at the South Pole is to be extended with radio antennaslike those tested at RNO-G.\n",
      "===================================\n",
      "Answer without RAG: DESY is a research center in Germany that is dedicated to particle physics. It is located in the city of Hamburg and is home to the world's largest particle accelerator, the LHC. DESY also houses a number of other facilities that are used to study the fundamental particles and forces that govern the universe.\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"glpat-QJocA6joz1gm7XD8rrRo\"\n",
    "API_URL = \"https://api.helmholtz-blablador.fz-juelich.de/v1/models\"  # Correct endpoint to list models\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_openai import ChatOpenAI\n",
    "import numpy as np\n",
    "from transformers import BitsAndBytesConfig\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "cache_dir = \"/afs/desy.de/user/t/taheri/scratch/cache\"\n",
    "\n",
    "#Optimize Model Loading and Memory Usage (3 steps)\n",
    "# 1: Clear GPU memory before loading the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Step 2: Load the Llama Model\n",
    "model_name = \"openlm-research/open_llama_3b\"\n",
    "#model_name = \"openlm-research/open_llama_3b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir) #, use_fast=False\n",
    "\n",
    "###device = \"cpu\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "###model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)  # Use FP16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "#2: 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Use FP16 for reduced memory usage\n",
    "    bnb_4bit_use_double_quant=True  # Further reduces memory usage\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #device_map=\"auto\",\n",
    "    device_map=device,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "#3 Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# Print the cache directory\n",
    "print(tokenizer.name_or_path)  # Model name\n",
    "\n",
    "\n",
    "def extract_and_clean_answer(response, split_key=\"Answer:\"):\n",
    "    \"\"\"\n",
    "    Extract and clean the answer from the response.\n",
    "    Removes duplicate lines, ensures the answer is not empty, and handles extra \"Context:\" or unwanted information.\n",
    "    \"\"\"\n",
    "    # Split the answer based on the provided split_key (e.g., \"Answer:\")\n",
    "    answer = response.split(split_key)[-1].strip()\n",
    "    \n",
    "    # Remove any lines that start with \"Context:\" or similar unwanted information\n",
    "    cleaned_answer = []\n",
    "    for line in answer.split(\"\\n\"):\n",
    "        #if not line.lower().startswith(\"context:\"):  # Remove lines starting with \"context:\"\n",
    "        if not line.lower().startswith((\"context:\", \"question:\")):  # Remove lines starting with \"context:\" or \"question:\"\n",
    "            cleaned_answer.append(line.strip())\n",
    "        \n",
    "          \n",
    "    \n",
    "    # Remove duplicate lines and reconstruct the answer\n",
    "    unique_lines = list(dict.fromkeys(cleaned_answer))  # Remove duplicate lines\n",
    "    return \"\\n\".join(unique_lines).strip()  # Reconstruct answer without repetition\n",
    "\n",
    "\n",
    "\n",
    "llama_model = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    #max_length=512,  # 128,256,512 for faster/slower inference & to control cost\n",
    "    max_new_tokens=500  #256 Ensures short answers\n",
    "    #temperature=0.5,  # [0-2] set randomness- increase for more creative answers like image/poem generator-decrease for more precise answer for fact based models\n",
    "    #top_p=1.0,  # [0-1] Ensures only high-probability tokens are used- It controls how deterministic the model is. \n",
    "    #top_k=10,\n",
    "    #do_sample=False,\n",
    "    #num_return_sequences=1 #,\n",
    "    #stop_sequence=\"\\n\"  # Stop generating after a newline\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#llm = HuggingFacePipeline(pipeline=llama_model)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")# , temperature=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Answer the question based on the context provided below. If the context does not contain the answer, say \"I don't know.\" \n",
    "- Make sure to check **all parts of the context** carefully before answering, even if the answer is spread across multiple sections.\n",
    "- Do **not** repeat words or phrases.\n",
    "- Provide a **complete and well-structured sentence** as your answer.\n",
    "- If the question asks for multiple points, provide a **list** or **detailed explanation**.\n",
    "- If the answer requires interpretation or synthesis of multiple pieces of information, ensure that the answer reflects the entire context accurately.\n",
    "- Do **not** repeat.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Build the RAG pipeline\n",
    "qa_with_data = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type= \"stuff\", # \"stuff\",  # Use \"map_reduce\": Step-by-Step Refinement or \"refine\"-> Step-by-Step Refinement for more complex tasks\n",
    "    #retriever=vectorstore.as_retriever(search_kwargs={\"k\": 10, \"score_threshold\": 0.25, \"search_type\": \"similarity\"}),  # Adjust k based on document length #score_threshold to filter out low-confidence retrievals\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 30, \"search_type\": \"mmr\"}), #, \"score_threshold\": 0.3- \"mmr\" , \"similarity\"  \n",
    "    return_source_documents=True , # Optionally return source documents for debugging\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def score_retrieved_documents(retrieved_docs, query_embedding):\n",
    "    scores = []\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)  # Normalize query embedding\n",
    "    for doc in retrieved_docs:\n",
    "        doc_embedding = embeddings.embed_query(doc.page_content)\n",
    "        doc_embedding = doc_embedding / np.linalg.norm(doc_embedding)  # Normalize document embedding\n",
    "        similarity = np.dot(query_embedding, doc_embedding)  # Cosine similarity\n",
    "        scores.append((doc, similarity))\n",
    "    return sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define the list of queries\n",
    "queries = [\n",
    "    \"How many employees does DESY have?\",\n",
    "    \"What are the main research fields at DESY?\",\n",
    "    \"Who is the Chairman of the DESY Board of Directors?\",\n",
    "    \"Who funds DESY, and how is it managed?\",\n",
    "    \"What is European XFEL, and how is DESY involved?\",\n",
    "    \"How does DESY contribute to particle physics?\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each query and get the response\n",
    "for query in queries:\n",
    "    #print(f\"Question: {query}\")\n",
    "    display(Markdown(f\"**Question: {query}**\"))\n",
    "    print(\"===================================\")\n",
    "\n",
    "        # Get the answer using RAG\n",
    "    result = qa_with_data({\"query\": query})\n",
    "       \n",
    "    answer_with_rag = extract_and_clean_answer(result.get('result', ''))\n",
    "    \n",
    "    print(f\"Answer with RAG: {answer_with_rag}\")\n",
    "\n",
    "    print(\"===================================\")\n",
    "\n",
    "    # Generate response without using RAG\n",
    "    formatted_prompt = PROMPT.format(context=\"No context available.\", question=query)\n",
    "    response_without_data = llama_model(formatted_prompt, num_return_sequences=1, do_sample=False) #, max_length=200, truncation=True, top_p=0.9\n",
    "\n",
    "    # Extract only the answer from the response\n",
    "    answer_without_rag = extract_and_clean_answer(response_without_data[0][\"generated_text\"])\n",
    "    \n",
    "    print(f\"Answer without RAG: {answer_without_rag}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c73f5-6352-473b-baca-7e6467b453b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964f3f3-469c-44b1-8265-46687a9cf539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b520921-c521-41fe-b813-da9b2a247478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be711a8b-9193-4b84-bed8-4ec81339057b",
   "metadata": {},
   "source": [
    "## chain_type=\"map_reduce\"\n",
    "\n",
    "#### map_reduced need to have two prompts- They need to be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5ce9642-4ccc-4640-9bc7-ed6265e1c3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openlm-research/open_llama_3b\n",
      "Question: How many employees does DESY have?\n",
      "===================================\n",
      "Answer with RAG: Approximately 3000\n"
     ]
    }
   ],
   "source": [
    "API_KEY = \"glpat-QJocA6joz1gm7XD8rrRo\"\n",
    "API_URL = \"https://api.helmholtz-blablador.fz-juelich.de/v1/models\"  # Correct endpoint to list models\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_openai import ChatOpenAI\n",
    "import numpy as np\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "cache_dir = \"/afs/desy.de/user/t/taheri/scratch/cache\"\n",
    "\n",
    "#Optimize Model Loading and Memory Usage (3 steps)\n",
    "# 1: Clear GPU memory before loading the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Step 2: Load the Llama Model\n",
    "model_name = \"openlm-research/open_llama_3b\"\n",
    "#model_name = \"openlm-research/open_llama_3b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir) #, use_fast=False\n",
    "\n",
    "###device = \"cpu\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "###model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)  # Use FP16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "#2: 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Use FP16 for reduced memory usage\n",
    "    bnb_4bit_use_double_quant=True  # Further reduces memory usage\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "#3 Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# Print the cache directory\n",
    "print(tokenizer.name_or_path)  # Model name\n",
    "\n",
    "\n",
    "llama_model = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    #max_length=512,  # 128,256,512 for faster/slower inference & to control cost\n",
    "    max_new_tokens=500  #256 Ensures short answers\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#llm = HuggingFacePipeline(pipeline=llama_model)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")# , temperature=1\n",
    "\n",
    "\n",
    "#========================================================\n",
    "\n",
    "\n",
    "question_prompt_template = \"\"\"Answer the question based on the context provided below. If the context does not contain the answer, say \"I don't know.\"\n",
    "- **If the context contains numerical or factual details, use them exactly.**\n",
    "- If the question asks for a number, return only the number if possible.\n",
    "- **Do not ignore any information in the provided context.**\n",
    "- If multiple pieces of information are available, synthesize them.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "Synthesize the answers from the following pieces of context to provide a final answer. If the context does not contain the answer, say \"I don't know.\"\n",
    "- Make sure to **synthesize** the information from all parts of the context.\n",
    "- Provide a **detailed answer**, including all numbers, quantities, or lists if mentioned.\n",
    "- If the question asks for multiple points, make sure to list them all or provide a comprehensive explanation.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Build the RAG pipeline\n",
    "qa_with_data = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",  # Use \"map_reduce\" for more complex tasks\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_kwargs={\n",
    "            \"k\": 30,  # Number of documents to retrieve\n",
    "            \"search_type\": \"mmr\"  # Use Maximal Marginal Relevance\n",
    "        }\n",
    "    ),\n",
    "    return_source_documents=True,  # Optionally return source documents for debugging\n",
    "    chain_type_kwargs={\n",
    "    \"question_prompt\": question_prompt,\n",
    "    \"combine_prompt\": combine_prompt,\n",
    "    \"combine_document_variable_name\": \"context\"  # Ensure correct variable name\n",
    "}\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Query the pipeline\n",
    "query = \"How many employees does DESY have?\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"===================================\")\n",
    "\n",
    "result = qa_with_data({\"query\": query})\n",
    "# Post-process the output to extract only the answer\n",
    "answer = result['result'].split(\"Answer:\")[-1].strip()\n",
    "if not result.get('result', None):\n",
    "    print(\"Sorry, I couldn't find an answer.\")\n",
    "else:\n",
    "    print(f\"Answer with RAG: {answer}\")\n",
    "\n",
    "#print(f\"Answer with RAG: {answer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a515a6-c289-4f8a-aa73-5777df64dd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5989fb6-bc89-46b2-9491-3ae3ea6d1a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11836b30-85c5-4209-a58f-a2b6b12d3c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b50b89-497d-46eb-8314-6bf0fd4a7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print retrieved documents\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i+1}: {doc.page_content[:200]}...\")  # Print the first 200 characters of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482816a-4a0e-4d09-8eff-174fdaf4f859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbc9db14-7a4a-4e49-833a-a03a165d07c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32mmain\u001b[m\n",
      "  master\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: a branch named 'main' already exists\n",
      "M\tDESY-IT-LLM-2025-02-21.ipynb\n",
      "A\tDESY-IT-LLM-2025-03-07.ipynb\n",
      "Already on 'main'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! git branch\n",
    "! git branch main  # Create a new branch called 'main'\n",
    "! git checkout main  # Switch to the 'main' branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce78d31-0b36-4c5b-962f-fef91ef71f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
